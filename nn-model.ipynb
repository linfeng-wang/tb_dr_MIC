{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f934b49aa10>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts_list(lst):\n",
    "    \"\"\"\n",
    "    Computes the frequency count of unique elements in a list and returns a dictionary, sorted by frequency count in\n",
    "    descending order.\n",
    "\n",
    "    Args:\n",
    "    - lst (list): List of elements\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with unique elements as keys and their frequency count as values, sorted by frequency count\n",
    "    in descending order\n",
    "    \"\"\"\n",
    "    value_counts = {}\n",
    "    for item in lst:\n",
    "        if item in value_counts:\n",
    "            value_counts[item] += 1\n",
    "        else:\n",
    "            value_counts[item] = 1\n",
    "    sorted_value_counts = dict(sorted(value_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_value_counts\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/gene_seq_train.csv')\n",
    "train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/res_train.csv')\n",
    "#don't touch test data, split out validation data from training data during training\n",
    "test_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/gene_seq_test.csv')\n",
    "test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/res_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = train_data.shape[0]\n",
    "DRUGS = ['AMIKACIN',\n",
    " 'CAPREOMYCIN',\n",
    " 'CIPROFLOXACIN',\n",
    " 'ETHAMBUTOL',\n",
    " 'ETHIONAMIDE',\n",
    " 'ISONIAZID',\n",
    " 'KANAMYCIN',\n",
    " 'LEVOFLOXACIN',\n",
    " 'MOXIFLOXACIN',\n",
    " 'OFLOXACIN',\n",
    " 'PYRAZINAMIDE',\n",
    " 'RIFAMPICIN',\n",
    " 'STREPTOMYCIN']\n",
    "\n",
    "DRUGS = train_target.columns\n",
    "LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.int8):\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    acgt_bytes = torch.ByteTensor(list(bytes(\"ACGT\", \"utf-8\")))\n",
    "    arr = torch.zeros(4, (len(seq_bytes)), dtype=dtype)\n",
    "    arr[0, seq_bytes == acgt_bytes[0]] = 1\n",
    "    arr[1, seq_bytes == acgt_bytes[1]] = 1\n",
    "    arr[2, seq_bytes == acgt_bytes[2]] = 1\n",
    "    arr[3, seq_bytes == acgt_bytes[3]] = 1\n",
    "    return arr\n",
    "\n",
    "# def one_hot_torch(seq):\n",
    "#     oh = []\n",
    "#     for sample in seq:\n",
    "#         sample = torch.ByteTensor(list(bytes(sample, \"utf-8\")))\n",
    "#         acgt_bytes = torch.ByteTensor(list(bytes(\"ACGT\", \"utf-8\")))\n",
    "#         arr = torch.zeros((len(sample), 4), dtype=torch.int8)\n",
    "#         arr[sample == acgt_bytes[0], 0] = 1\n",
    "#         arr[sample == acgt_bytes[1], 1] = 1\n",
    "#         arr[sample == acgt_bytes[2], 2] = 1\n",
    "#         arr[sample == acgt_bytes[3], 3] = 1\n",
    "#         oh.append(arr)\n",
    "#     return torch.stack(oh)\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_masked_loss(loss_fn):\n",
    "    \"\"\"\n",
    "    Returns a loss function that ignores NaN values\n",
    "    \"\"\"\n",
    "\n",
    "    def masked_loss(y_true, y_pred):\n",
    "        y_pred = y_pred.view(-1, 13)  # Ensure y_pred has the same shape as y_true and non_nan_mask\n",
    "        # ic(y_pred.shape)\n",
    "        # ic(y_true.shape)\n",
    "        non_nan_mask = ~y_true.isnan()\n",
    "        # ic(non_nan_mask)\n",
    "        y_true_non_nan = y_true[non_nan_mask]\n",
    "        y_pred_non_nan = y_pred[non_nan_mask]\n",
    "        # ic(y_true)\n",
    "        # ic(y)\n",
    "        # ic(y_true_non_nan)\n",
    "        # ic(y_pred_non_nan)\n",
    "        return loss_fn(y_pred_non_nan, y_true_non_nan)\n",
    "\n",
    "    return masked_loss\n",
    "\n",
    "masked_MSE = get_masked_loss(torch.nn.MSELoss())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julian's code - implement this, might be faster\n",
    "class OneHotSeqsDataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "    ):\n",
    "        self.seq_df = seq_df[target_loci]\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        if not self.seq_df.index.equals(self.res_df.index):\n",
    "            raise ValueError(\n",
    "                \"Indices of sequence and resistance dataframes don't match up\"\n",
    "            )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df.iloc[index].str.cat()\n",
    "            res = self.res_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df.loc[index].str.cat()\n",
    "            res = self.res_df.loc[index]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "        return one_hot_torch(seqs_comb, dtype=self.one_hot_dtype), torch.tensor(res)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "    \n",
    "training_dataset = OneHotSeqsDataset(train_data, train_target, one_hot_dtype=torch.float)\n",
    "train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.8), len(training_dataset)-int(len(training_dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class seq_dataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#     def __getitem__(self, index):\n",
    "#         seqs_comb = self.x.iloc[index,1:].str.cat(sep='X'*30)\n",
    "#         seqs_comb = one_hot_torch(seqs_comb)\n",
    "#         seqs_comb = seqs_comb.permute(2, 0, 1).contiguous().view(4, 57350)\n",
    "#         res = torch.as_tensor(self.y.iloc[index,:].values.tolist())\n",
    "        \n",
    "#         return seqs_comb, res\n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "\n",
    "# training_dataset = seq_dataset(train_data, train_target)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.8), len(training_dataset)-int(len(training_dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = Model(in_channel = 4, hidden_channel = 6, out_channel=13, batch_size=batch_size)\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "\n",
    "epoch = 20\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = masked_MSE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channel = 4, hidden_channel = 64, out_channel=13, batch_size=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        # self.feature_extraction = nn.Conv1d(in_channels, hidden, kernel_size=kernel_size),\n",
    "        self.conv1 = nn.Conv1d(in_channel, hidden_channel, kernel_size=25, stride=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_channel, hidden_channel, kernel_size=25, stride=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc = None\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # ic('c1',x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # ic('c2',x.size())\n",
    "        \n",
    "        x = F.relu(self.pool(x))\n",
    "        # ic('pool',x.size()) \n",
    "        x = torch.max(x, dim=1).values\n",
    "        # ic('max',x.size())\n",
    "        # print('x after maxpool:', x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('x after view:', x.size())\n",
    "\n",
    "        if self.fc is None:\n",
    "            # Set the input size based on x the first time forward is called\n",
    "            # x = x.T\n",
    "            input_size = x.size(1)\n",
    "            self.fc = nn.Linear(input_size, 13).to(x.device)  # Ensure the layer is on the same device as x\n",
    "        # print(x.size())\n",
    "\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        x = torch.squeeze(x)\n",
    "        # print('output:', x)\n",
    "        # print('1111')             \n",
    "        # first_dim_size = x.size(0)\n",
    "        # x = x.reshape(first_dim_size, -1).contiguous()\n",
    "        # first_dim_size = x.size(0)\n",
    "        # print('size after research', x.size())\n",
    "        return x\n",
    "\n",
    "model = Model(in_channel = 4, hidden_channel = 8, out_channel=13, batch_size=batch_size)\n",
    "model = model.float()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "ic.disable()\n",
    "# ic.enable()\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    for x, y in train_loader:\n",
    "        x_batch = torch.squeeze(x, 0).to(device)\n",
    "        y_batch = y.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        # y_batch = y_batch.view(-1)\n",
    "\n",
    "        # y_batch = one_hot_torch(y).to(device)\n",
    "        # print('batch y size before flatten:',y_batch.size())\n",
    "        # y_batch = y_batch.flatten()\n",
    "        # print('batch y size after flatten:',y_batch.size())\n",
    "        # print(x_batch.size())\n",
    "        # print(x_batch.size())\n",
    "# For example, if you have a convolutional layer with 64 output channels, 3 input channels, and a kernel size of 3x3, the weight parameters would have a dimension of (64, 3, 3, 3)\n",
    "        # print(x_batch.size())\n",
    "        pred = model(x_batch.float())\n",
    "        # print(x_batch)\n",
    "        # print(pred)\n",
    "        # pred = pred.unsqueeze(0)\n",
    "        # ic(pred)\n",
    "        # ic(y_batch)\n",
    "        loss_train = criterion(y_batch, pred)\n",
    "        ic(loss_train)\n",
    "        train_batch_loss.append(loss_train)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    with torch.no_grad():\n",
    "        # print('test')\n",
    "        for x, y in test_loader:\n",
    "            x_batch = x.to(device)\n",
    "            y_batch = y.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            # pred = pred.unsqueeze(0)\n",
    "\n",
    "            loss_test = criterion(y_batch, pred)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017],\n",
    "                    [0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                     0.5677, 0.4784, 0.4777, 0.5017]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 13])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846,\n",
    "                            0.5677, 0.4784, 0.4777, 0.5017, 0.5542, 0.4820, 0.4521, 0.4912, 0.5167,\n",
    "                            0.5015, 0.4697, 0.4547, 0.4846, 0.5677, 0.4784, 0.4777, 0.5017, 0.5542,\n",
    "                            0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846, 0.5677,\n",
    "                            0.4784, 0.4777, 0.5017, 0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015,\n",
    "                            0.4697, 0.4547, 0.4846, 0.5677, 0.4784, 0.4777, 0.5017, 0.5542, 0.4820,\n",
    "                            0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846, 0.5677, 0.4784,\n",
    "                            0.4777, 0.5017, 0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697,\n",
    "                            0.4547, 0.4846, 0.5677, 0.4784, 0.4777, 0.5017, 0.5542, 0.4820, 0.4521,\n",
    "                            0.4912, 0.5167, 0.5015, 0.4697, 0.4547, 0.4846, 0.5677, 0.4784, 0.4777,\n",
    "                            0.5017, 0.5542, 0.4820, 0.4521, 0.4912, 0.5167, 0.5015, 0.4697, 0.4547,\n",
    "                            0.4846, 0.5677, 0.4784, 0.4777, 0.5017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_true = torch.Tensor([2.0000e-01, 6.0000e-02, 2.0000e-02, 3.0000e-02, 1.0000e+00, 2.0000e+00,\n",
    "                            5.0000e-02, 2.0000e+00, 5.0000e-01, 5.0000e-01, 2.5000e-01, 2.0000e-02,\n",
    "                            3.0000e-02,        nan, 1.2000e-01, 5.0000e-01, 6.0000e-02, 5.0000e-01,\n",
    "                            2.0000e+00,        nan, 4.0000e+00, 5.0000e-01, 1.0000e+00,        nan,\n",
    "                            1.2000e-01, 3.0000e-02, 2.0000e-01, 6.0000e-02, 2.0000e-02,        nan,\n",
    "                                   nan, 2.0000e+00,        nan, 4.0000e+00, 2.5000e-01,        nan,\n",
    "                            1.2000e-01, 2.0000e-02, 3.0000e-02,        nan, 6.0000e-02,        nan,\n",
    "                            5.0000e-03, 1.0000e+00, 4.0000e+00, 5.0000e-02, 2.0000e+00, 2.5000e-01,\n",
    "                                   nan, 2.5000e-01, 1.2000e-01, 3.0000e-02, 2.0000e-01,        nan,\n",
    "                                   nan, 5.0000e-03, 8.0000e+00, 2.0000e+00,        nan, 5.0000e-01,\n",
    "                                   nan, 2.5000e-01,        nan, 5.0000e-01,        nan, 2.0000e-01,\n",
    "                            3.0000e-02, 2.0000e-02, 3.0000e-02, 1.0000e+00, 1.0000e+00, 5.0000e-02,\n",
    "                            2.0000e+00, 4.0000e+00, 5.0000e-01, 4.0000e+00, 1.2000e-01, 3.0000e-02,\n",
    "                            2.0000e-01, 1.5000e-02, 6.0000e-02, 3.0000e-02,        nan, 4.0000e+00,\n",
    "                            4.0000e-01, 2.0000e+00, 2.5000e-01,        nan, 2.5000e-01, 1.0000e+01,\n",
    "                            3.0000e+00, 2.0000e-01, 5.0000e-03, 2.0000e-02, 5.0000e-03, 2.0000e+00,\n",
    "                            1.0000e+00, 1.6000e+00, 5.0000e-01, 1.0000e+00, 1.2000e-01, 5.0000e-01,\n",
    "                            5.0000e+00, 1.0000e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "#%%\n",
    "a = torch.zeros(1, 2, 3, 4, 5, 6)\n",
    "b = a.view(a.shape[:2], -1, a.shape[5:])\n",
    "# %%\n",
    "# from torchviz import make_dot\n",
    "# x = torch.randn(2, 4, 56).to(device)\n",
    "# m = model_torch_simple.raw_seq_model().to(device)\n",
    "# y = m(x)\n",
    "# make_dot(y, params=dict(list(m.named_parameters()))).render(\"cnn_torchviz\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-g1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
